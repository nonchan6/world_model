{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"v1SLWpzDZ5cq"},"outputs":[],"source":["import time\n","import numpy as np\n","import torch\n","from torch.distributions import Normal\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# tensorboard用\n","from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TDrrWLgZ5cu"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","source":["## 0.目次\n","\n","1. それぞれの手札の状況から、世界モデルを獲得する部分（RSSM、Dreamer）\n","2. 共通の盤面の状況から、遷移や価値を求める部分（RNN、Dreamer）\n","3. 1, 2から行動を求めたり、2と行動から1を予測したりする部分（条件付きVAE）\n","4. Agentが行動を決定する部分\n","5. 補助機能の実装"],"metadata":{"id":"PWEmXMZ3yUKD"}},{"cell_type":"markdown","metadata":{"id":"cabFV6m9Z5cv"},"source":["## 1.世界モデルの獲得\n","\n","演習第5回のRSSM、Dreamerを参考にしつつ、以下を実装する。\n","\n","* TransitionModel\n"," * reccurent : 状態遷移\n"," * prior : 状態遷移を用いた1ステップ先の未来の状態表現の分布\n"," * posterior : 1ステップ先の観測の情報を取り込んで計算した状態表現の分布\n","* ObservationModel : 観測を復元するデコーダ\n","* EncoderModel : 観測から低次元へ\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zF4myzdAZ5cx"},"source":["以下のプログラム上の変数と意味\n","* state (s_t) : priorやposteriorから構成される確率的状態(mean:平均、stddev:標準偏差で再パラメータ化)\n","* action (a_t) : 行動\n","* rnn_hidden (h_t) : 状態遷移\n","* embedded_obs (e_t) : 観測を低次元(64次元)にしたもの\n","* obs (o_t) : 観測\n","* hidden : 計算するときの中間層(全て32次元)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hv7_mdouZ5cx"},"outputs":[],"source":["class TransitionModel(nn.Module):\n","    \"\"\"\n","    自分の状況を表す世界モデル\n","    決定的状態遷移(RNN) : h_t+1 = f(h_t, s_t, a_t)\n","    確率的状態遷移による1ステップ予測として定義される \"prior\" : p(s_t+1 | h_t+1)\n","    観測の情報を取り込んで定義される \"posterior\": q(s_t+1 | h_t+1, e_t+1)\n","    \"\"\"\n","    def __init__(self, state_dim, action_dim, rnn_hidden_dim, hidden_dim=32, min_stddev=0.1, act=F.elu):\n","        super(TransitionModel, self).__init__()\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.fc_state_action = nn.Linear(state_dim + action_dim, hidden_dim)\n","      \n","        self.fc_rnn_hidden = nn.Linear(rnn_hidden_dim, hidden_dim)\n","        self.fc_state_mean_prior = nn.Linear(hidden_dim, state_dim)\n","        self.fc_state_stddev_prior = nn.Linear(hidden_dim, state_dim)\n","\n","        self.fc_rnn_hidden_embedded_obs = nn.Linear(rnn_hidden_dim + 64, hidden_dim)\n","        self.fc_state_mean_posterior = nn.Linear(hidden_dim, state_dim)\n","        self.fc_state_stddev_posterior = nn.Linear(hidden_dim, state_dim)\n","\n","        #next hidden stateを計算\n","        self.rnn = nn.GRUCell(hidden_dim, rnn_hidden_dim)\n","        self._min_stddev = min_stddev\n","        self.act = act\n","  \n","\n","    def forward(self, state, action, rnn_hidden, embedded_next_obs):\n","        \"\"\"\n","        h_t+1 = f(h_t, s_t, a_t)\n","        prior p(s_t+1 | h_t+1) と posterior q(s_t+1 | h_t+1, e_t+1) を返す\n","        この2つが近づくように学習する\n","        \"\"\"\n","        next_state_prior, rnn_hidden = self.prior(self.reccurent(state, action, rnn_hidden))\n","        next_state_posterior = self.posterior(rnn_hidden, embedded_next_obs)\n","        return next_state_prior, next_state_posterior, rnn_hidden\n","      \n","    def reccurent(self, state, action, rnn_hidden):\n","        \"\"\"\n","        h_t+1 = f(h_t, s_t, a_t)を計算する\n","        \"\"\"\n","        hidden = self.act(self.fc_state_action(torch.cat([state, action], dim=1)))\n","        #h_t+1を求める\n","        rnn_hidden = self.rnn(hidden, rnn_hidden)\n","        return rnn_hidden\n","\n","    def prior(self, rnn_hidden):\n","        \"\"\"\n","        prior p(s_t+1 | h_t+1) を計算する\n","        \"\"\"\n","        #h_t+1を求める\n","        hidden = self.act(self.fc_rnn_hidden(rnn_hidden))\n","\n","        mean = self.fc_state_mean_prior(hidden)\n","        stddev = F.softplus(self.fc_state_stddev_prior(hidden)) + self._min_stddev\n","        return Normal(mean, stddev), rnn_hidden\n","\n","    def posterior(self, rnn_hidden, embedded_obs):\n","        \"\"\"\n","        posterior q(s_t+1 | h_t+1, e_t+1)  を計算する\n","        \"\"\"\n","        # h_t+1, o_t+1を結合し, q(s_t+1 | h_t+1, e_t+1) を計算する\n","        hidden = self.act(self.fc_rnn_hidden_embedded_obs(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n","        mean = self.fc_state_mean_posterior(hidden)\n","        stddev = F.softplus(self.fc_state_stddev_posterior(hidden)) + self._min_stddev\n","        return Normal(mean, stddev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TmIeMLaZ5c0"},"outputs":[],"source":["class ObservationModel(nn.Module):\n","    \"\"\"\n","    p(o_t | s_t, h_t)\n","    低次元の状態表現から画像を再構成するデコーダ （23次元: 手札の状態の次元）\n","    \"\"\"\n","    def __init__(self, state_dim, rnn_hidden_dim, hidden_dim=32):\n","        super(ObservationModel, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, 64)\n","        self.fc2 = nn.Linear(64, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, 23)\n","\n","\n","    def forward(self, state, rnn_hidden):\n","        hidden = self.relu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = self.relu(self.fc2(hidden))\n","        obs = self.fc3(hidden)\n","        return obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eVE6XAJZ5c1"},"outputs":[],"source":["class EncoderModel(nn.Module):\n","    \"\"\"\n","    p(e_t | o_t)\n","    状態を低次元ベクトルに変換するエンコーダ\n","    \"\"\"\n","    def __init__(self, hidden_dim=32):\n","        super(EncoderModel, self).__init__()\n","        self.fc1 = nn.Linear(23, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, 64)\n","\n","    def forward(self, obs):\n","        hidden = F.relu(self.fc1(obs))\n","        hidden = F.relu(self.fc2(hidden))\n","        embedded_obs = F.relu(self.fc3(hidden))\n","        return embedded_obs"]},{"cell_type":"markdown","metadata":{"id":"Vrw6QTOtZ5c2"},"source":["上記で定義された3つのモデルを`World`クラスとしてまとめる。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PDIUsCOZ5c2"},"outputs":[],"source":["class World:\n","    def __init__(self, state_dim, action_dim, rnn_hidden_dim):\n","        self.transition = TransitionModel(state_dim, action_dim, rnn_hidden_dim).to(device)\n","        self.observation = ObservationModel(state_dim, rnn_hidden_dim).to(device)\n","        self.encoder = EncoderModel().to(device)"]},{"cell_type":"markdown","source":["## 2.盤面の状態遷移\n","\n","演習第5回のRSSM、Dreamerを参考にしつつ、以下を実装する。\n","\n","* RnnModel : 状態遷移を行うクラス\n","* EmbeddingModel : 観測から低次元に変換するクラス\n","* RewardModel : 報酬を予測するクラス\n","* (5/23削除)ValueModel : 価値関数を計算するクラス\n","\n"],"metadata":{"id":"h_ou7puGfn-D"}},{"cell_type":"markdown","source":["以下のプログラム上の変数と意味\n","\n","* state (s_t) : 状態\n","* action (a_t) : 行動\n","* rnn_hidden (h_t) : 状態遷移\n","* obs (o_t) : 観測（盤面の状態）\n","* hidden : 計算するときの中間層(全て32次元)"],"metadata":{"id":"xuzj158ZiV6r"}},{"cell_type":"code","source":["class RnnModel(nn.Module):\n","    \"\"\"\n","    盤面に関する状態遷移\n","    決定的状態遷移(RNN) : h_t+1 = f(h_t, s_t, a_t)\n","    次の状態を予測 : p(s_t+1, h_t+1)\n","    \"\"\"\n","    def __init__(self, state_dim, action_dim, rnn_hidden_dim, hidden_dim=32, min_stddev=0.1, act=F.elu):\n","        super(RnnModel, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n","        self.state_rnn = nn.GRUCell(hidden_dim, rnn_hidden_dim)\n","        self.fc2 = nn.Linear(rnn_hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, state_dim)\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.act = act\n","  \n","\n","    def forward(self, state, action, rnn_hidden):\n","        # h_t+1を求める\n","        hidden = self.act(self.fc1(torch.cat([state, action], dim=1)))\n","        rnn_hidden = self.state_rnn(hidden, rnn_hidden)\n","        return rnn_hidden\n","    \n","    def prior(self, rnn_hidden):\n","        # s_tを求める\n","        hidden = self.act(self.fc2(rnn_hidden))\n","        next_state = self.fc3(hidden)\n","        return next_state"],"metadata":{"id":"SZ0ktnUVhLPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EmbeddingModel(nn.Module):\n","    \"\"\"\n","    p(e_t | o_t)\n","    状態を低次元ベクトルに変換するエンコーダ\n","    \"\"\"\n","    def __init__(self, hidden_dim=32):\n","        super(EmbeddingModel, self).__init__()\n","        self.fc1 = nn.Linear(23, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, state_dim)\n","\n","    def forward(self, obs):\n","        hidden = F.relu(self.fc1(obs))\n","        hidden = F.relu(self.fc2(hidden))\n","        state = F.relu(self.fc3(hidden))\n","        return state"],"metadata":{"id":"SvOR8Ihds4wQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RewardModel(nn.Module):\n","    \"\"\"\n","    p(r_t | s_t, h_t) \n","    低次元の状態表現から報酬を予測する\n","    \"\"\"\n","    def __init__(self, state_dim, rnn_hidden_dim, hidden_dim=32, act=F.elu):\n","        super(RewardModel, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","        self.act = act\n"," \n","\n","    def forward(self, state, rnn_hidden, action):\n","        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","        reward = self.fc4(hidden)\n","        return reward"],"metadata":{"id":"NK4WpYCRRFka"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記で定義された3つのモデルを`State`クラスとしてまとめる。"],"metadata":{"id":"f2fTGNtCu0hB"}},{"cell_type":"code","source":["class State:\n","    def __init__(self, state_dim, action_dim, rnn_hidden_dim):\n","        self.rnn = RnnModel(state_dim, action_dim, rnn_hidden_dim).to(device)\n","        self.embedding = EmbeddingModel().to(device)\n","        self.reward = RewardModel(state_dim, rnn_hidden_dim).to(device)"],"metadata":{"id":"ZQ04qMTPu6oU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.行動と世界モデルの予測\n","\n","条件付きVAEを参考にしつつ、以下を実装する。\n","(5/23補足)条件付きVAEは参考にしない。\n","\n","* ActionModel : 盤面状態と世界モデルから行動を選択する\n","* DecorderModel : 盤面状態と行動から世界モデルを予測する\n","* (5/23追加)ValueModel :  盤面状態と世界モデルから価値を予測する"],"metadata":{"id":"qhKEae1dvpMk"}},{"cell_type":"markdown","source":["以下のプログラム上の変数と意味\n","\n","* state: 盤面状態（条件付きVAEのy）\n","* rnn_hidden: 盤面状態遷移（条件付きVAEのy）\n","* world : 世界モデル（条件付きVAEのx）\n","* action : 行動（条件付きVAEのz）\n","* hidden : 隠れ層(64次元)\n","\n","※ スライドと、xとzを逆にして定義している。"],"metadata":{"id":"bH0R7fmnw_nX"}},{"cell_type":"code","source":["class ActionModel(nn.Module):\n","    \"\"\"\n","    世界モデル(world_dim)と低次元の状態表現(state_dim + rnn_hidden_dim)から行動を計算するクラス\n","    \"\"\"\n","    def __init__(self, world_dim, state_dim, rnn_hidden_dim, action_dim,\n","                 hidden_dim=64, act=F.elu, min_stddev=1e-4, init_stddev=5.0):\n","        super(ActionModel, self).__init__()\n","        self.fc1 = nn.Linear(world_dim + state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc_mean = nn.Linear(hidden_dim, action_dim)\n","        self.fc_stddev = nn.Linear(hidden_dim, action_dim)\n","        self.act = act\n","        self.min_stddev = min_stddev\n","        self.init_stddev = np.log(np.exp(init_stddev) - 1)\n","\n","    def forward(self, world, state, rnn_hidden, training=True):\n","        \"\"\"\n","        training=Trueなら, NNのパラメータに関して微分可能な形の行動のサンプル（Reparametrizationによる）を返します\n","        training=Falseなら, 行動の確率分布の平均値を返します\n","        \"\"\"\n","        hidden = self.act(self.fc1(torch.cat([world, state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","\n","        # Dreamerの実装に合わせて少し平均と分散に対する簡単な変換が入っています\n","        mean = self.fc_mean(hidden)\n","        mean = 5.0 * torch.tanh(mean / 5.0)\n","        stddev = self.fc_stddev(hidden)\n","        stddev = F.softplus(stddev + self.init_stddev) + self.min_stddev\n","\n","        if training:\n","            action = torch.tanh(Normal(mean, stddev).rsample()) # 微分可能にするためrsample()\n","        else:\n","            action = torch.tanh(mean)\n","        # lossの計算で必要なmeanとstddevも追加で返す\n","        return mean, stddev, action"],"metadata":{"id":"ia99xCd1C_pV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderModel(nn.Module):\n","    \"\"\"\n","    行動の世界モデル(action_dim)と低次元の状態表現(state_dim + rnn_hidden_dim)から世界モデルを計算するクラス\n","    \"\"\"\n","    def __init__(self, world_dim, state_dim, rnn_hidden_dim, action_dim,\n","                hidden_dim=32, act=F.elu, min_stddev=1e-4, init_stddev=5.0):\n","        super(DecoderModel, self).__init__()\n","        self.fc1 = nn.Linear(action_dim + state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, world_dim)\n"," \n","\n","    def forward(self, state, rnn_hidden, action):\n","        hidden = self.act(self.fc1(torch.cat([action, state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","        world = self.act(self.fc4(hidden))\n","        return world"],"metadata":{"id":"U0uNo3gNEt9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ValueModel(nn.Module):\n","    \"\"\"\n","    低次元の状態表現から状態価値を出力する\n","    \"\"\"\n","    def __init__(self, world_dim, state_dim, rnn_hidden_dim, hidden_dim=32, act=F.elu):\n","        super(ValueModel, self).__init__()\n","        self.fc1 = nn.Linear(world_dim + state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","        self.act = act\n","\n","    def forward(self, world, state, rnn_hidden):\n","        hidden = self.act(self.fc1(torch.cat([world, state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","        state_value = self.fc4(hidden)\n","        return state_value"],"metadata":{"id":"vCKCvXeW_SFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記で定義された3つのモデルを`CVAE`クラスとしてまとめる。"],"metadata":{"id":"kcme74zgJ0BE"}},{"cell_type":"code","source":["class CVAE():\n","    def __init__(self, world_dim, state_dim, rnn_hidden_dim, action_dim):\n","        self.action = ActionModel(world_dim, state_dim, rnn_hidden_dim, action_dim).to(device)\n","        self.decoder = DecoderModel(world_dim, state_dim, rnn_hidden_dim, action_dim).to(device)\n","        self.value = ValueModel(world_dim, state_dim, rnn_hidden_dim).to(device)"],"metadata":{"id":"JGu6la5bw9XX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.エージェント\n","\n","第5回演習7章を参考にし、以下を実装する。\n","\n","* MyAgent : 自分の行動を決定するクラス。手札の状況が入力として与えられ、1、2章で学習した低次元の状態表現を計算し、行動を決定する。\n","* EnemyAgent : 相手の行動を予測するクラス。今までの状態と行動から相手の世界モデルの状態遷移を元に、1、2章で学習した低次元の状態表現を計算し、行動を決定する。\n","\n","7章でMyAgentを用いて学習し、8章で学習したモデルをEnemyAgentに適応し検証する。"],"metadata":{"id":"alcPDoi0MN77"}},{"cell_type":"markdown","source":["以下のプログラム上の変数と意味\n","\n","* world : 1章で作成したクラス\n","* state : 2章で作成したクラス\n","* cvae : 3章で作成したクラス\n","* obs : 盤面の観測\n","* hand : 自分の手札の観測"],"metadata":{"id":"89NgnblYXUIY"}},{"cell_type":"code","source":["class MyAgent:\n","    \"\"\"\n","    自分の行動を決定するクラス\n","    \"\"\"\n","    def __init__(self, world, state, cvae):\n","        self.world = world\n","        self.state = state \n","        self.cvae = cvae\n","        \n","        self.device = next(cvae.action.parameters()).device\n","        self.world_rnn_hidden = torch.zeros(1, world.transition.rnn_hidden_dim, device=self.device)\n","        self.state_rnn_hidden = torch.zeros(1, state.rnn.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, hand, training=True):\n","        obs = torch.as_tensor(obs, device=self.device)\n","        hand = torch.as_tensor(hand, device=self.device)\n","\n","        with torch.no_grad():\n","            # 手札\n","            embedded_hand = self.world.encoder(hand)\n","            state_posterior = self.world.transition.posterior(self.world_rnn_hidden, embedded_hand)\n","            world = state_posterior.sample()\n","            # 盤面\n","            embedded_obs = self.state.embedding(obs)\n","            state = embedded_obs.sample()\n","            # 行動選択\n","            action = self.cvae.action(world, state, self.state_rnn_hidden, training=training)\n","\n","            # 次のステップのために隠れ状態を更新しておく\n","            self.world_rnn_hidden = self.world.transition.reccurent(world, action, self.world_rnn_hidden)\n","            self.state_rnn_hidden = self.state.rnn(state, action, self.state_rnn_hidden)\n","\n","        return action.squeeze().cpu().numpy()\n","\n","    #RNNの隠れ状態をリセット\n","    def reset(self):\n","        self.world_rnn_hidden = torch.zeros(1, world.transition.rnn_hidden_dim, device=self.device)\n","        self.state_rnn_hidden = torch.zeros(1, state.rnn.rnn_hidden_dim, device=self.device)"],"metadata":{"id":"QUbal9wlMQ4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EnemyAgent:\n","    def __init__(self, world, state, cvae):\n","        self.world = world\n","        self.state = state\n","        self.cvae = cvae\n","\n","        self.device = next(cvae.action.parameters()).device\n","        self.world_rnn_hidden = torch.zeros(1, world.transition.rnn_hidden_dim, device=self.device)\n","        self.state_rnn_hidden = torch.zeros(1, state.rnn.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, training=True):\n","        \"\"\"\n","        相手の行動を予測するクラス\n","        \"\"\"\n","        obs = torch.as_tensor(obs, device=self.device)\n","\n","        with torch.no_grad():\n","            # 世界モデル\n","            world = self.world.transition.prior(self.world_rnn_hidden)\n","            world = world.sample()\n","            # 盤面\n","            embedded_obs = self.state.embedding(obs)\n","            state = embedded_obs.sample()\n","            # 行動選択\n","            # でも、何手まで読めばいいのか分からない。\n","            action = self.cvae.action(world, state, self.state_rnn_hidden, training=training)\n","\n","            # 次のステップのために隠れ状態を更新しておく\n","            # predictのworld_rnn_hiddenと同じにしないほうがいい。\n","            # 相手の行動が実際に行われたら、正しい情報に更新しないといけないから。\n","            self.world_rnn_hidden = self.world.transition.reccurent(world, action, self.world_rnn_hidden)\n","            self.state_rnn_hidden = self.state.rnn(state, action, self.state_rnn_hidden)\n","\n","        return action.squeeze().cpu().numpy()\n","    \n","    def predict(self, obs, action):\n","        \"\"\"\n","        相手が行動するたび、相手の世界モデルを更新するために、この関数を実行する\n","        \"\"\"\n","        obs = torch.as_tensor(obs, device=self.device)\n","        action = torch.as_tensor(action, device=self.device)\n","\n","        with torch.no_grad():\n","            # 盤面\n","            embedded_obs = self.state.embedding(obs)\n","            state = embedded_obs.sample()\n","            # 相手の世界モデル\n","            world = self.cvae.decoder(state, self.state_rnn_hidden, action)\n","\n","            # 次のステップのために隠れ状態を更新しておく\n","            self.world_rnn_hidden = self.world.transition.reccurent(world, action, self.world_rnn_hidden)\n","            self.state_rnn_hidden = self.state.rnn(state, action, self.state_rnn_hidden)\n","\n","\n","    # RNNの隠れ状態をリセット\n","    def reset(self):\n","        self.world_rnn_hidden = torch.zeros(1, world.rnn_hidden_dim, device=self.device)\n","        self.state_rnn_hidden = torch.zeros(1, state.rnn_hidden_dim, device=self.device)"],"metadata":{"id":"sBpiTwD3cjwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_fYvfNBZ5c3"},"source":["## 5.補助機能\n","\n","第5回演習5章を参考にし、以下を実装する。\n","* リプレイバッファ（myhandsを追加しただけ）\n","* λ-returnを計算する関数（編集なし）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrOYQv9jZ5c6"},"outputs":[],"source":["# 今回のReplayBuffer\n","class ReplayBuffer(object):\n","    \"\"\"\n","    RNNを用いて訓練するのに適したリプレイバッファ\n","    \"\"\"\n","    def __init__(self, capacity, observation_shape, hand_shape, action_dim):\n","        self.capacity = capacity\n","\n","        self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n","        self.myhands = np.zeros((capacity, *hand_shape), dtype=np.uint8)\n","        # 敵の手札の情報は、世界モデルの学習には使用しない。\n","        # self.enemyhands = np.zeros((capacity, *hand_shape), dtype=np.uint8)\n","        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n","        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n","        self.done = np.zeros((capacity, 1), dtype=np.bool)\n","\n","        self.index = 0\n","        self.is_filled = False\n","\n","    def push(self, observation, myhand, action, reward, done):\n","        \"\"\"\n","        リプレイバッファに経験を追加する\n","        \"\"\"\n","        self.observations[self.index] = observation\n","        self.myhands[self.index] = myhand\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.done[self.index] = done\n","\n","        # indexは巡回し, 最も古い経験を上書きする\n","        if self.index == self.capacity - 1:\n","            self.is_filled = True\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size, chunk_length):\n","        \"\"\"\n","        経験をリプレイバッファからサンプルします. （ほぼ）一様なサンプルです\n","        結果として返ってくるのは観測(画像), 行動, 報酬, 終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です\n","        各バッチは連続した経験になっています\n","        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります\n","        \"\"\"\n","        episode_borders = np.where(self.done)[0]\n","        sampled_indexes = []\n","        for _ in range(batch_size):\n","            cross_border = True\n","            while cross_border:\n","                initial_index = np.random.randint(len(self) - chunk_length + 1)\n","                final_index = initial_index + chunk_length - 1\n","                cross_border = np.logical_and(initial_index <= episode_borders,\n","                                              episode_borders < final_index).any() # 論理積\n","            sampled_indexes += list(range(initial_index, final_index + 1))\n","\n","        sampled_observations = self.observations[sampled_indexes].reshape(\n","            batch_size, chunk_length, *self.observations.shape[1:])\n","        sampled_myhands = self.myhands[sampled_indexes].reshape(\n","            batch_size, chunk_length, *self.myhands.shape[1:])\n","        sampled_actions = self.actions[sampled_indexes].reshape(\n","            batch_size, chunk_length, self.actions.shape[1])\n","        sampled_rewards = self.rewards[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        sampled_done = self.done[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        return sampled_observations, sampled_myhands, sampled_actions, sampled_rewards, sampled_done\n","\n","    def __len__(self):\n","        return self.capacity if self.is_filled else self.index"]},{"cell_type":"code","source":["def lambda_target(rewards, values, gamma, lambda_):\n","    \"\"\"\n","    価値関数の学習のためのλ-returnを計算します\n","    \"\"\"\n","    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n","\n","    H = rewards.shape[0] - 1\n","    V_n = torch.zeros_like(rewards, device=rewards.device)\n","    V_n[H] = values[H]\n","    for n in range(1, H+1):\n","        # まずn-step returnを計算します\n","        # 注意: 系列が途中で終わってしまったら, 可能な中で最大のnを用いたn-stepを使います\n","        V_n[:-n] = (gamma ** n) * values[n:]\n","        for k in range(1, n+1):\n","            if k == n:\n","                V_n[:-n] += (gamma ** (n-1)) * rewards[k:]\n","            else:\n","                V_n[:-n] += (gamma ** (k-1)) * rewards[k:-n+k]\n","\n","        # lambda_でn-step returnを重みづけてλ-returnを計算します\n","        if n == H:\n","            V_lambda += (lambda_ ** (H-1)) * V_n\n","        else:\n","            V_lambda += (1 - lambda_) * (lambda_ ** (n-1)) * V_n\n","\n","    return V_lambda"],"metadata":{"id":"CBNzDO_ZXzOt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYyymdITZ5c_"},"source":["## 6.パラメータの設定\n","\n","第5回演習8章を参考にし、実装する。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjMFFrw_Z5c_"},"outputs":[],"source":["# リプレイバッファの宣言\n","buffer_capacity = 200000  # Colabのメモリの都合上, 元の実装より小さめにとっています\n","replay_buffer = ReplayBuffer(capacity=buffer_capacity, \n","                             observation_shape=(1, 23),\n","                             hand_shape=(1, 23),\n","                             action_dim=3*13)\n","\n","# モデルの次元設定\n","world_dim = 16  # 確率的状態（世界モデル）の次元\n","state_dim = 16  # 盤面状態を圧縮した次元\n","action_dim = 23  # 行動の次元\n","rnn_hidden_dim = 64  # 決定的状態（RNNの隠れ状態）の次元\n","# モデルの宣言\n","world = World(world_dim, action_dim, rnn_hidden_dim)\n","state = State(state_dim, action_dim, rnn_hidden_dim)\n","cvae = CVAE(world_dim, state_dim, rnn_hidden_dim, action_dim)\n","\n","\n","# 学習率の設定\n","world_lr = 6e-4\n","state_lr = 6e-4\n","cvae_lr = 8e-5\n","eps = 1e-4\n","world_params = (list(world.transition.parameters()) +\n","                list(world.observation.parameters()) +\n","                list(world.encoder.parameters()))\n","state_params = (list(state.rnn.parameters()) +\n","                list(state.embedding.parameters())+\n","                list(state.reward.parameters()))\n","world_optimizer = torch.optim.Adam(world_params, lr=world_lr, eps=eps)\n","state_optimizer = torch.optim.Adam(state_params, lr=state_lr, eps=eps)\n","action_optimizer = torch.optim.Adam(cvae.action.parameters(), lr=action_lr, eps=eps)\n","decoder_optimizer = torch.optim.Adam(cvae.decoder.parameters(), lr=action_lr, eps=eps)\n","value_optimizer = torch.optim.Adam(cvae.value.parameters(), lr=value_lr, eps=eps)\n","\n","# その他ハイパーパラメータ\n","# 以下未編集\n","seed_episodes = 5  # 最初にランダム行動で探索するエピソード数\n","all_episodes = 100  # 学習全体のエピソード数（300ほどで, ある程度収束します）\n","test_interval = 10  # 何エピソードごとに探索ノイズなしのテストを行うか\n","model_save_interval = 20  # NNの重みを何エピソードごとに保存するか\n","collect_interval = 100  # 何回のNNの更新ごとに経験を集めるか（＝1エピソード経験を集めるごとに何回更新するか）\n","\n","action_noise_var = 0.3  # 探索ノイズの強さ\n","\n","batch_size = 50\n","chunk_length = 50  # 1回の更新で用いる系列の長さ\n","imagination_horizon = 15  # Actor-Criticの更新のために, Dreamerで何ステップ先までの想像上の軌道を生成するか\n","\n","\n","gamma = 0.9  # 割引率\n","lambda_ = 0.95  # λ-returnのパラメータ\n","clip_grad_norm = 100  # gradient clippingの値\n","free_nats = 3  # KL誤差（RSSMのTransitionModelにおけるpriorとposteriorの間の誤差）がこの値以下の場合, 無視する"]},{"cell_type":"markdown","metadata":{"id":"yUoI0l9Ee0Tp"},"source":["## 7.自分の世界モデルの学習\n","\n","第5回演習9章を参考にし、実装する。\n","\n","ここでは自分の世界モデルを獲得し、良い行動選択ができるようになることを目標とする。"]},{"cell_type":"markdown","source":["### 7.1 Dreamerの学習ループ\n","```\n","5回最後まで実行し、経験を貯める。\n","\n","1エポックごとに1回最後まで実行し、100回NNの更新を行う。\n","\n","* RSSMの更新\n"," * リプレイバッファから、バッチサイズ50*系列長さ50 の経験を読み込む\n"," * 系列長さの50回、次の状態をpriorとposteriorで予測し、klダイバージェンスを計算\n"," * 観測を再構成し、実際の観測との平均二乗誤差を計算\n"," * 報酬を予測し、環境から得た報酬との平均二乗誤差を計算\n"," * 誤差を全て足してRSSMとEncoderの更新\n","\n","* ActionModel, ValueModelの更新\n"," * 15ステップ分、確率的状態を入力とする学習可能な行動モデルで、想像上の軌道を作成\n"," * 架空の軌道に対する報酬と価値を計算し、λ-returnも計算\n"," * 更新した価値関数で求めた価値が大きくなるように、行動モデルの平均二乗誤差を計算し、行動モデルを更新\n"," * 価値関数とλ-returnの平均二乗誤差を計算し、価値関数を更新\n","\n","10エポックごとに探索ノイズなしでテストをし、20エポックごとにモデルを保存する。\n","```"],"metadata":{"id":"g5InzjHV5laE"}},{"cell_type":"markdown","source":["### 7.2 今回のモデルの学習ループ\n","```\n","5回最後まで実行し、経験を貯める。\n","\n","1エポックごとに1回最後まで実行し、100回NNの更新を行う。\n","\n","* World（Transition, Observation, Encoder）の更新\n"," * リプレイバッファから、バッチサイズ50*系列長さ50 の経験を読み込む\n"," * 系列長さの50回、次の状態をpriorとposteriorで予測し、klダイバージェンスを計算\n"," * 観測を再構成し、実際の観測との平均二乗誤差を計算\n"," * 誤差を全て足してWorldの更新\n","\n","* State（RNN, Enbedding, Reward）の更新\n"," * リプレイバッファから、バッチサイズ50*系列長さ50 の経験を読み込む\n"," * RNNで予測した状態と低次元の状態との平均二乗誤差を計算\n"," * 報酬と環境から得た報酬との平均二乗誤差を計算\n"," * 誤差を全て足してStateの更新\n","\n","* CVAE（Action, Decoder, Value）の更新\n"," * 15ステップ分、確率的状態を入力とする学習可能な行動モデルで、想像上の軌道を作成\n"," * 架空の軌道に対する報酬と価値を計算し、λ-returnも計算 ？？？\n"," * 更新した価値関数で求めた価値が大きくなるように、行動モデルの平均二乗誤差を計算し、行動モデルを更新 ？？？\n"," * 世界モデルを再構成し、実際の世界モデルとの平均二乗誤差を計算\n"," * 価値関数とλ-returnの平均二乗誤差を計算し、価値関数を更新\n","\n","10エポックごとに探索ノイズなしでテストをし、20エポックごとにモデルを保存する。\n","```"],"metadata":{"id":"KmWkiW3DWfy-"}},{"cell_type":"markdown","source":["### 7.3 問題点\n","\n","1つ目\n","```\n","Dreamerの報酬は、状態のみから報酬がもらえたが、今回の環境は、行動後に行った行動によって報酬がもらえる。\n","つまり、確率的状態のみから報酬r(s)を求めることは不可能で、報酬を予測するのに、状態と行動を用いるように変更する必要がある。\n","\n","また、価値関数もv(s)ではなく、行動価値関数q(s,a)を学習しなければいけない。行動が離散的ということも考慮しなければならない。\n","\n","さらに、ActionModelを求める方法も、λ-returnにマイナスをつけたものではなく、Actor-criticを参考にして、新たに考える必要がある。\n","（lambda_targetがどういう計算をしているのか、まだ詳しく理解できていない）\n","```\n","\n","\n","2つ目\n","```\n","CVAEのzは、xを再構成できるという性質をもつものであるので、zには正しい行動を選択できるという性質はない。\n","そのため、zから行動を選択できるように、さらに層を追加するべきだと思う。\n","\n","というか、そもそもCVAEを使わず、盤面の状態と世界モデルから、全結合で行動を選択すればいいかも。\n","```\n"],"metadata":{"id":"vmg-yPzCkpCX"}},{"cell_type":"markdown","source":["## 7.4 実装"],"metadata":{"id":"qdqV2Ji_cvfh"}},{"cell_type":"code","source":["# -------------------------------------------------------------------------------------\n","#  5回最後まで実行し、経験を貯める\n","# -------------------------------------------------------------------------------------\n","env = make_env()\n","for episode in range(seed_episodes):\n","    env.reset()\n","    decision_steps, terminal_steps = env.get_steps(behavior_name)\n","    tracked_agent = decision_steps.agent_id[0]\n","    done = False\n","\n","    while not done:\n","        # 環境から観測を入手\n","        decision_steps, terminal_steps = env.get_steps(behavior_name)\n","        obs = decision_steps.obs\n","        myhand = decision_steps.obs  # <=================ここ変更\n","\n","        # ランダムに行動選択\n","        action = spec.action_spec.random_action(len(decision_steps))\n","        # 環境の更新\n","        env.set_actions(behavior_name, action)\n","        env.step()\n","\n","        # 環境から終了かどうかの判別\n","        decision_steps, terminal_steps = env.get_steps(behavior_name)\n","        if tracked_agent in decision_steps:\n","            reward = decision_steps[tracked_agent].reward\n","            episode_reward += reward\n","        if tracked_agent in terminal_steps:\n","            reward = terminal_steps[tracked_agent].reward\n","            episode_reward += reward\n","            done=True\n","        \n","        # 記録\n","        replay_buffer.push(obs, myhand, action, reward, done)"],"metadata":{"id":"_XwhO7qrcyvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = make_env()\n","for episode in range(seed_episodes, all_episodes):\n","    start = time.time()\n","\n","    # -------------------------------------------------------------------------------------\n","    #  1エポックごとに1回最後まで実行\n","    # -------------------------------------------------------------------------------------\n","    # 行動を決定するためのエージェントを宣言\n","    policy = MyAgent(world, state, cvae)  # まだ\n","    # 環境のリセット\n","    env.reset()\n","    decision_steps, terminal_steps = env.get_steps(behavior_name)\n","    tracked_agent = decision_steps.agent_id[0]\n","    done = False\n","    episode_reward = 0\n","    while not done:\n","        # 環境から観測を入手\n","        decision_steps, terminal_steps = env.get_steps(behavior_name)\n","        obs = decision_steps.obs\n","        myhand = decision_steps.obs  # <=================ここ変更\n","\n","        # 行動選択をし、ノイズを加える。\n","        action = policy(obs, myhand)\n","        action += np.random.normal(0, np.sqrt(action_noise_var), action.shape)\n","        # 環境の更新\n","        env.set_actions(behavior_name, action)\n","        env.step()\n","        \n","        # 環境から終了かどうかの判別\n","        decision_steps, terminal_steps = env.get_steps(behavior_name)\n","        if tracked_agent in decision_steps:\n","            reward = decision_steps[tracked_agent].reward\n","            episode_reward += reward\n","        if tracked_agent in terminal_steps:\n","            reward = terminal_steps[tracked_agent].reward\n","            episode_reward += reward\n","            done=True\n","\n","        # 記録\n","        replay_buffer.push(obs, myhand, action, reward, done)\n","\n","    # 訓練時の報酬と経過時間をログとして表示\n","    writer.add_scalar('total reward at train', episode_reward, episode)\n","    print('episode [%4d/%4d] is collected. Total reward is %f' %\n","            (episode+1, all_episodes, episode_reward))\n","    print('elasped time for interaction: %.2fs' % (time.time() - start))\n","\n","\n","    # -------------------------------------------------------------------------------------\n","    #  100回NNの更新を行う\n","    # -------------------------------------------------------------------------------------\n","    start = time.time()\n","    for update_step in range(collect_interval):\n","        # -------------------------------------------------------------------------------------\n","        #  World（Transition, Observation, Encoder）の更新\n","        # -------------------------------------------------------------------------------------\n","        # リプレイバッファから、バッチサイズ50*系列長さ50 の経験を読み込む\n","        observations, myhands, actions, rewards, _ = replay_buffer.sample(batch_size, chunk_length)\n","\n","        # 観測の前処理\n","        observations = torch.as_tensor(obs, device=device)\n","        myhands = torch.as_tensor(myhands, device=device)\n","        actions = torch.as_tensor(actions, device=device)\n","        rewards = torch.as_tensor(rewards, device=device)\n","\n","        # 観測をエンコーダで低次元のベクトルに変換\n","        embedded_hand = world.encoder(myhands).view(chunk_length, batch_size, -1)\n","\n","        # 低次元の状態表現を保持しておくためのTensorを定義\n","        worlds = torch.zeros(chunk_length, batch_size, world_dim, device=device)\n","        world_rnn_hiddens = torch.zeros(chunk_length, batch_size, rnn_hidden_dim, device=device)\n","        # 低次元の状態表現は最初はゼロ初期化\n","        world = torch.zeros(batch_size, world_dim, device=device)\n","        world_rnn_hidden = torch.zeros(batch_size, rnn_hidden_dim, device=device)\n","\n","        # 系列長さの50回、次の状態をpriorとposteriorで予測し、klダイバージェンスを計算\n","        kl_loss = 0\n","        for l in range(chunk_length-1):\n","            next_state_prior, next_state_posterior, world_rnn_hidden = \\\n","                world.transition(world, actions[l], world_rnn_hidden, embedded_hand[l+1])\n","            world = next_state_posterior.rsample()\n","            worlds[l+1] = world\n","            world_rnn_hiddens[l+1] = world_rnn_hidden\n","            kl = kl_divergence(next_state_prior, next_state_posterior).sum(dim=1)\n","            kl_loss += kl.clamp(min=free_nats).mean()  # 原論文通り, KL誤差がfree_nats以下の時は無視\n","        kl_loss /= (chunk_length - 1)\n","\n","        # states[0] and rnn_hiddens[0]はゼロ初期化なので以降では使わない\n","        worlds = worlds[1:]\n","        world_rnn_hiddens = world_rnn_hiddens[1:]\n","\n","        # 観測を再構成し、実際の観測との平均二乗誤差を計算\n","        flatten_worlds = worlds.view(-1, world_dim)\n","        flatten_world_rnn_hiddens = world_rnn_hiddens.view(-1, rnn_hidden_dim)\n","        recon_myhands = world.observation(flatten_worlds, flatten_world_rnn_hiddens).view(chunk_length-1, batch_size, 3, 23)\n","        hand_loss = 0.5 * F.mse_loss(recon_myhands, myhands[:-1], reduction='none').mean([0, 1]).sum()\n","\n","        # 誤差を全て足してWorldの更新\n","        world_loss = kl_loss + hand_loss\n","        world_optimizer.zero_grad()\n","        world_loss.backward()\n","        clip_grad_norm_(world_params, clip_grad_norm)\n","        world_optimizer.step()\n","\n","        # 勾配の流れを遮断\n","        flatten_worlds = flatten_worlds.detach()\n","        flatten_world_rnn_hiddens = flatten_world_rnn_hiddens.detach()\n","\n","        # -------------------------------------------------------------------------------------\n","        #  State（RNN, Enbedding）の更新\n","        # -------------------------------------------------------------------------------------\n","\n","        # 観測をエンコーダで低次元のベクトルに変換\n","        embedded_obs = state.embedding(observations).view(chunk_length, batch_size, -1)\n","\n","        # 低次元の状態表現を保持しておくためのTensorを定義\n","        states = torch.zeros(chunk_length, batch_size, state_dim, device=device)\n","        state_rnn_hiddens = torch.zeros(chunk_length, batch_size, rnn_hidden_dim, device=device)\n","        # 低次元の状態表現は最初はゼロ初期化\n","        state = torch.zeros(batch_size, state_dim, device=device)\n","        state_rnn_hidden = torch.zeros(batch_size, rnn_hidden_dim, device=device)\n","\n","        # RNNを実行\n","        for l in range(chunk_length-1):\n","            state_rnn_hidden = state.rnn(state, actions[l], state_rnn_hidden)\n","            next_state = state.rnn.prior(state_rnn_hidden)\n","            state = embedded_obs[l]  # 盤面の低次元状態は観測を圧縮したもの\n","            states[l+1] = state\n","            state_rnn_hiddens[l+1] = state_rnn_hidden\n","        \n","        # states[0] and rnn_hiddens[0]はゼロ初期化なので以降では使わない\n","        states = states[1:]\n","        state_rnn_hiddens = state_rnn_hiddens[1:]\n","\n","        flatten_states = states.view(-1, state_dim)\n","        flatten_state_rnn_hiddens = state_rnn_hiddens.view(-1, rnn_hidden_dim)\n","        flatten_actions = actions[:-1].view(-1, action_dim)\n","\n","        # RNNで予測した状態と低次元の状態との平均二乗誤差を計算\n","        recon_observations = state.rnn.prior(flatten_state_rnn_hiddens).view(chunk_length-1, batch_size, 3, 23)\n","        obs_loss = 0.5 * F.mse_loss(recon_observations, flatten_states)\n","  \n","        # 報酬を予測し、環境から得た報酬との平均二乗誤差を計算\n","        predicted_rewards = state.reward(flatten_states, flatten_state_rnn_hiddens).view(chunk_length-1, batch_size, 1)\n","        reward_loss = 0.5 * F.mse_loss(predicted_rewards, rewards[:-1])\n","\n","        # 勾配降下で更新する\n","        state_loss = obs_loss + reward_loss\n","        state_optimizer.zero_grad()\n","        state_loss.backward()\n","        clip_grad_norm_(state_params, clip_grad_norm)\n","        state_optimizer.step()\n","\n","        # 勾配の流れを遮断\n","        flatten_states = flatten_states.detach()\n","        flatten_state_rnn_hiddens = flatten_state_rnn_hiddens.detach()\n","\n","        # -------------------------------------------------------------------------------------\n","        #  CVAE（Action, Decoder, Value）の更新\n","        # -------------------------------------------------------------------------------------\n","\n","        # DreamerにおけるActor-Criticの更新のために, 現在のモデルを用いた数ステップ先の未来の状態予測を保持するためのTensorを用意\n","        imaginated_worlds = torch.zeros(imagination_horizon + 1, *flatten_worlds.shape, device=flatten_worlds.device)\n","        imaginated_world_rnn_hiddens = torch.zeros(imagination_horizon + 1, *flatten_world_rnn_hiddens.shape, device=flatten_world_rnn_hiddens.device)\n","        imaginated_states = torch.zeros(imagination_horizon + 1, *flatten_states.shape, device=flatten_states.device)\n","        imaginated_state_rnn_hiddens = torch.zeros(imagination_horizon + 1, *flatten_state_rnn_hiddens.shape, device=flatten_state_rnn_hiddens.device)\n","        imaginated_actions = torch.zeros(imagination_horizon + 1, *flatten_actions.shape, device=flatten_actions.device)\n","\n","        # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n","        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n","        imaginated_worlds[0] = flatten_worlds\n","        imaginated_world_rnn_hiddens[0] = flatten_world_rnn_hiddens\n","        imaginated_states[0] = flatten_states\n","        imaginated_state_rnn_hiddens[0] = flatten_state_rnn_hiddens\n","        imaginated_actions[0] = flatten_actions\n","        \n","        # open-loopで未来の状態予測を使い, 想像上の軌道を作る\n","        for h in range(1, imagination_horizon + 1):\n","            # 行動はActionModelで決定. この行動はモデルのパラメータに対して微分可能で, これを介してActionModelは更新される\n","            actions = cvae.action(flatten_worlds, flatten_states, flatten_state_rnn_hiddens)\n","            flatten_world_prior, flatten_world_rnn_hiddens = \\\n","                world.transition.prior(world.transition.reccurent(flatten_worlds, actions, flatten_world_rnn_hiddens))\n","            flatten_state_rnn_hiddens = state.rnn(flatten_states, actions, flatten_state_rnn_hiddens)\n","            flatten_state_prior = state.rnn.prior(flatten_state_rnn_hiddens)\n","\n","            flatten_worlds = flatten_world_prior.rsample()\n","            flatten_states = flatten_state_prior.rsample()\n","            imaginated_worlds[h] = flatten_worlds\n","            imaginated_world_rnn_hiddens[h] = flatten_world_rnn_hiddens\n","            imaginated_states[h] = flatten_states\n","            imaginated_state_rnn_hiddens[h] = flatten_state_rnn_hiddens\n","            imaginated_actions[h] = actions\n","\n","        # RSSMのreward_modelにより予測された架空の軌道に対する報酬を計算\n","        flatten_imaginated_worlds = imaginated_worlds.view(-1, world_dim)\n","        flatten_imaginated_world_rnn_hiddens = imaginated_world_rnn_hiddens.view(-1, rnn_hidden_dim)\n","        flatten_imaginated_states = imaginated_states.view(-1, state_dim)\n","        flatten_imaginated_state_rnn_hiddens = imaginated_state_rnn_hiddens.view(-1, rnn_hidden_dim)\n","        flatten_imaginated_actions = imaginated_actions.view(-1, action_dim)\n","        imaginated_rewards = state.reward(flatten_imaginated_states, flatten_imaginated_state_rnn_hiddens).view(imagination_horizon + 1, -1)\n","        imaginated_values = cvae.value(flatten_imaginated_worlds, flatten_imaginated_states, flatten_imaginated_state_rnn_hiddens).view(imagination_horizon + 1, -1)\n","\n","        # λ-returnのターゲットを計算(V_{\\lambda}(s_{\\tau})\n","        lambda_target_values = lambda_target(imaginated_rewards, imaginated_values, gamma, lambda_)\n","        \n","        # 価値関数の予測した価値が大きくなるようにActionModelを更新. PyTorchの基本は勾配降下だが, 今回は大きくしたいので-1をかける\n","        action_loss = -lambda_target_values.mean()\n","        action_optimizer.zero_grad()\n","        action_loss.backward()\n","        clip_grad_norm_(cvae.action.parameters(), clip_grad_norm)\n","        action_optimizer.step()\n","\n","        # 世界モデルを再構成し、実際の世界モデルとの平均二乗誤差を計算\n","        decoder_world = cvae.decoder(flatten_imaginated_states.detach(), flatten_imaginated_state_rnn_hiddens.detach(), flatten_imaginated_actions.detach())\n","        decoder_loss = -lambda_target_values.mean()\n","        decoder_optimizer.zero_grad()\n","        decoder.backward()\n","        clip_grad_norm_(cvae.decoder.parameters(), clip_grad_norm)\n","        decoder_optimizer.step()\n","\n","        # TD(λ)ベースの目的関数で価値関数を更新（価値関数のみを学習するため，学習しない変数のグラフは切っている. )\n","        imaginated_values = cvae.value(flatten_imaginated_worlds.detach(), flatten_imaginated_states.detach(), flatten_imaginated_state_rnn_hiddens.detach()).view(imagination_horizon + 1, -1)        \n","        value_loss = 0.5 * F.mse_loss(imaginated_values, lambda_target_values.detach())\n","        value_optimizer.zero_grad()\n","        value_loss.backward()\n","        clip_grad_norm_(cvae.value.parameters(), clip_grad_norm)\n","        value_optimizer.step()\n","\n","        # ログをTensorBoardに出力\n","        print('update_step: %3d world_loss: %.5f (kl_loss: %.5f + hand_loss: %.5f),'\n","              'state_loss: %.5f (obs_loss: %.5f + reward_loss: %.5f),'\n","              'action_loss: %.5f, decoder_loss: %.5f value_loss: %.5f'\n","            % (update_step + 1, world_loss.item(), kl_loss.item(), hand_loss.item(), \n","               state_loss.item(), obs_loss.item(), reward_loss.item(), action_loss.item(), decoder_loss.item(), value_loss.item())))\n","        \n","        total_update_step = episode * collect_interval + update_step\n","        writer.add_scalar('world loss', world_loss.item(), total_update_step)\n","        writer.add_scalar('kl loss', kl_loss.item(), total_update_step)\n","        writer.add_scalar('hand loss', hand_loss.item(), total_update_step)\n","        writer.add_scalar('state loss', state_loss.item(), total_update_step)\n","        writer.add_scalar('obs loss', obs_loss.item(), total_update_step)\n","        writer.add_scalar('reward loss', reward_loss.item(), total_update_step)\n","        writer.add_scalar('action loss', action_loss.item(), total_update_step)\n","        writer.add_scalar('decoder loss', decoder_loss.item(), total_update_step)\n","        writer.add_scalar('value loss', value_loss.item(), total_update_step)\n","\n","    print('elasped time for update: %.2fs' % (time.time() - start))\n","\n","\n","    # -------------------------------------------------------------------------------------\n","    #  10エポックごとに探索ノイズなしでテスト\n","    # -------------------------------------------------------------------------------------\n","    \"\"\"\n","    if (episode + 1) % test_interval == 0:\n","        policy = Agent(encoder, rssm.transition, action_model)\n","        start = time.time()\n","        obs = env.reset()\n","        done = False\n","        total_reward = 0\n","        while not done:\n","            action = policy(obs, training=False)\n","            obs, reward, done, _ = env.step(action)\n","            total_reward += reward\n","\n","        writer.add_scalar('total reward at test', total_reward, episode)\n","        print('Total test reward at episode [%4d/%4d] is %f' %\n","                (episode+1, all_episodes, total_reward))\n","        print('elasped time for test: %.2fs' % (time.time() - start))\n","    \"\"\"\n","    # -------------------------------------------------------------------------------------\n","    #  20エポックごとにモデルを保存\n","    # -------------------------------------------------------------------------------------\n","    \"\"\"\n","    if (episode + 1) % model_save_interval == 0:\n","        model_log_dir = os.path.join(log_dir, 'episode_%04d' % (episode + 1))\n","        os.makedirs(model_log_dir)\n","        torch.save(encoder.state_dict(), os.path.join(model_log_dir, 'encoder.pth'))\n","        torch.save(rssm.transition.state_dict(), os.path.join(model_log_dir, 'rssm.pth'))\n","        torch.save(rssm.observation.state_dict(), os.path.join(model_log_dir, 'obs_model.pth'))\n","        torch.save(rssm.reward.state_dict(), os.path.join(model_log_dir, 'reward_model.pth'))\n","        torch.save(value_model.state_dict(), os.path.join(model_log_dir, 'value_model.pth'))\n","        torch.save(action_model.state_dict(), os.path.join(model_log_dir, 'action_model.pth'))\n","    \"\"\""],"metadata":{"id":"bcc9FmIGf6k1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8.相手の世界モデルを予測して対戦"],"metadata":{"id":"dOSf9nzy5UWV"}}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"name":"world_model_0520.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}